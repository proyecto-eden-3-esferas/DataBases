[
{
  "name": "Wayback Machine",
  "pronounciation": "/weɪbæk maˈʃin/",
  "lang": "es",
  "definition": "un servicio y una base de datos que contiene copias de una enorme cantidad de páginas o sitios de Internet. Como consecuencia de este proyecto, también se puede consultar la historia y las modificaciones de las páginas a través del tiempo"
},

{
  "term": "parallax (effects)",
  "lang": "en",
  "field": "computing",
  "definition": "a visual design technique where a website's foreground images move at a different speed or direction than the background. Scrolling reveals subtle depth changes, drawing attention to images and text as you move down the page"
},

{
  "term":  "character",
  "lang":  "en",
  "field": "computing",
  "definition": "a basic unit of information that represents a letter, number, punctuation, space, symbol, or control code; it can be a single alphanumeric character like \"A\" or a special character like \"$\" or \"& \"",
  "instances": ["a", "7", ";", " "]
},
{
  "term":  "code point",
  "lang":  "en",
  "field": "computing",
  "definition": "a number assigned to represent an abstract character in a system for representing text (such as Unicode)",
  "notes": [
    "In Unicode, a code point is expressed in the form \"U+1234\" where \"1234\" is the assigned number. For example, the character \"A\" is assigned a code point of U+0041",
    "Character encoding forms, such as UTF-8 and UTF-16, determine how a Unicode code point should be encoded as a sequence of bytes. Different encoding forms may encode the same code point as different byte sequences: for example, the Cyrillic character \"Ф\", whose code point is U+0424, is encoded as 0xd0a4 in UTF-8 and as 0x0424 in UTF-16."
  ]
},
{
  "term":  "string",
  "lang":  "en",
  "field": "computing",
  "definition": "a run of characters",
  "notes": [
    "strings are usually delimeted by double quotes, less often by single quotes (standard SQL), back ticks etc."
  ],
  "instances": [
    "\"The highest summit\"",
    "\"Mary's choice\""
  ]
},
{
  "term":  "globbing",
  "lang":  "en",
  "field": "computing",
  "definition": "a name pattern becoming expanded into a list of names matching that pattern",
  "notes": []
},
{
  "term":  "escape character",
  "lang":  "en",
  "field": "computing",
  "definition": "a character that invokes an alternative interpretation on the following characters in a character sequence",
  "instances": [],
  "functions": [
    "to encode a syntactic entity, such as device commands or special data, which cannot be directly represented by the alphabet",
    "to represent characters, referred to as \"character quoting\", which cannot be typed in the current context, or would have an undesired interpretation; in this case, an escape sequence is a digraph consisting of an escape character itself and a \"quoted\" character"
  ],
  "notes": [
    "In the telecommunications field, escape characters are used to indicate that the following characters are encoded differently. This is used to alter control characters that would otherwise be noticed and acted on by the underlying telecommunications hardware, such as illegal characters. In this context, the use of escape characters is often referred to as quoting."
  ]
},

{
  "term": "operating system",
  "lang": "en",
  "field": "computing",
  "definition": "a collection of software that manages a computer's hardware and applications by allocating resources, including memory, CPU, input/output devices and file storage",
  "functions": [
    {"name": "managing computer hardware"},
    {"name": "running applications"},
    {"name": "enabling users to communicate with computers through a user-friendly interface"},
    {"name": "processor management", "definition": "allocating various tasks to a processor and giving it enough time to function"},
    {"name": "memory management", "definition": "allocating and deallocating memory across different processes"},
    {"name": "device management", "definition": "controlling the workings of input-output devices, including receiving and communicating requests, performing tasks and interacting with peripheral devices such as printers and cameras"},
    {"name": "file management", "definition": "tracking information that supports file system storage and maintaining integrity of data"},
    {"name": "security", "definition": "ensuring the confidentiality and integrity of data, including protecting against unauthorised or malicious access and relaying vulnerabilities"},
    {"name": "error detection", "definition": "checking for external threats, malicious software and hardware changes"},
    {"name": "job scheduling", "definition": "determining which applications need to run in which order"}
  ],
  "components": [
  ],
  "instances": [
    {"name": "Microsoft Windows", "notes": ["the most widely used global operating system, the market leader of desktop OS since the early 1990s"]},
    {"name": "Android"},
    {"name": "Linux"},
    {"name": "macOS"},
    {"name": "iOS",
     "notes": [
       "the second most widely used mobile OS",
       "iOS was developed solely for use with Apple hardware devices such as the iPhone, iPad, iPod and other handheld devices"
     ]},
    {"name": "ChromeOS",
     "notes": [
      "developed for use with tablets and netbooks",
      "an intuitive, secure and straightforward OS"
     ]}
  ],
  "kinds": [
    {"name": "Multi-tasking operating system"},
    {"name": "Multi-processing operating system"},
    {"name": "Time-sharing operating system"},
    {"name": "Real-time operating system"},
    {"name": "Multi-programming batch operating system"},
    {"name": "Distributed operating system"},
    {"name": "Network operating system"},
    {"name": "Simple batch operating system"},
    {"name": "Mobile operating system  "}
  ]
},


{
  "term": "buffer",
  "lang": "en",
  "field": "computing",
  "definition": null
},
{
  "term": "asynchronous",
  "lang": "en",
  "field": "computing",
  "definition": null
},
{
  "term": "concurrent",
  "lang": "en",
  "field": "computing",
  "definition": null
},

{
  "term": "process",
  "lang": "en",
  "field": "computing",
  "subfield": "operating systems",
  "definition": null
},

{
  "term": "thread",
  "lang": "en",
  "field": "computing",
  "subfield": "operating systems",
  "definition": null
},

{
  "term": "dirty bit",
  "lang": "en",
  "field": "computing",
  "definition": "a flag used to indicate whether a block of memory or data has been modified since it was last written to a slower storage medium (like disk)",
  "uses": [
    "caching",
    "virtual memory management"
  ]
},
{
  "term":  "ignored escaping",
  "lang":  "en",
  "field": "computing",
  "definition": "escaping that neither honoured nor disrupts parsing",
  "instances": []
},
{
  "term": "flush",
  "lang": "en",
  "field": "computing",
  "definition": "the process of clearing out or cleaning data from a buffer or cache"
},

{
  "term": "adware",
  "lang": "en",
  "field": "computing",
  "definition": "software with, often unwanted, adverts",
  "notes": [ ]
},

{
  "term": "ethereum",
  "lang": "en",
  "field": null,
  "definition": "decentralised mining network and software development platform rolled into one",
  "source": "Vitalik Buterin"
},

{
  "term": "blockchain",
  "field": "computing",
  "definition": "a cryptographic transaction ledger"
},

{
  "term": "symbolic artificial intelligence",
  "synonyms": ["logic-based artificial intelligence"],
  "field": "artificial intelligence",
  "definition": "the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search",
  "tools": ["logic programming", "production rules", "semantic nets", "frames"],
  "applications-developed": ["knowledge-based systems (in particular, expert systems)", "symbolic mathematics", "automated theorem provers", "ontologies", "the semantic web", "automated planning", "scheduling systems"],
  "notes": [
    "The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems"
  ],
  "history": [
    "Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field.[citation needed] An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.",
    "Neural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012. Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation. However, since 2020, as inherent difficulties with bias, explanation, comprehensibility, and robustness became more apparent with deep learning approaches; an increasing number of AI researchers have called for combining the best of both the symbolic and neural network approaches and addressing areas that both approaches have difficulty with, such as common-sense reasoning."
  ]
}

]
